%% ****** Start of file apstemplate.tex ****** %
%%
%% This file is part of the APS files in the REVTeX 4 distribution.
%% Version 4.1r of REVTeX, August 2010
%%
%%
%% Copyright (c) 2001, 2009, 2010 The American Physical Society.
%%
%% See the REVTeX 4 README file for restrictions and more information.
%%
%
% This is a template for producing manuscripts for use with REVTEX 4.0
% Copy this file to another name and then work on that file.
% That way, you always have this original template file to use.
%
% Group addresses by affiliation; use superscriptaddress for long
% author lists, or if there are many overlapping affiliations.
% For Phys. Rev. appearance, change preprint to twocolumn.
% Choose pra, prb, prc, prd, pre, prl, prstab, prstper, or rmp for journal
%  Add 'draft' option to mark overfull boxes with black boxes
%  Add 'showpacs' option to make PACS codes appear
%  Add 'showkeys' option to make keywords appear
\documentclass[aps,pra,reprint,longbibliography,superscriptaddress]{revtex4-1}
%\documentclass[aps,prl,preprint,superscriptaddress]{revtex4-1}
%\documentclass[aps,prl,reprint,groupedaddress]{revtex4-1}

% You should use BibTeX and apsrev.bst for references
% Choosing a journal automatically selects the correct APS
% BibTeX style file (bst file), so only uncomment the line
% below if necessary.
%\bibliographystyle{apsrev4-1}
\usepackage{graphicx}
\usepackage{amssymb, amsmath}
\usepackage{esdiff} % use this package for derivative and partial derivate commands of \diff{}{} and \diffp{}{}
\usepackage{siunitx}
\usepackage{float}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{newfloat}
\usepackage{xr}

\DeclareSIUnit\cells{cells}
\DeclareMathOperator{\hilbert}{\mathcal{H}} % To define additional named operators
\DeclareMathOperator{\h}{h} % To define additional named operators
\DeclareMathOperator{\AUC}{AUC} % To define additional named operators
\DeclareMathOperator{\cost}{cost} % To define additional named operators
\DeclareMathOperator{\unwrap}{unwrap} % To define additional named operators
\DeclareMathOperator{\vercat}{cat_\downarrow} % To define additional named operators
\DeclareMathOperator{\horcat}{cat_\rightarrow} % To define additional named operators
% \DeclareFloatingEnvironment[name={Supplementary Figure}]{suppfigure}
\externaldocument[S-]{supp}

\begin{document}

% Use the \preprint command to place your local institutional report
% number in the upper righthand corner of the title page in preprint mode.
% Multiple \preprint commands are allowed.
% Use the 'preprintnumbers' class option to override journal defaults
% to display numbers if necessary
%\preprint{}

%Title of paper
\title{Deep Learning in Label-free Cell Classification}

% repeat the \author .. \affiliation  etc. as needed
% \email, \thanks, \homepage, \altaffiliation all apply to the current
% author. Explanatory text should go in the []'s, actual e-mail
% address or url should go in the {}'s for \email and \homepage.
% Please use the appropriate macro foreach each type of information

% \affiliation command applies to all authors since the last
% \affiliation command. The \affiliation command should follow the
% other information
% \affiliation can be followed by \email, \homepage, \thanks as well.
\author{Claire L. Chen}
\email[Email: ]{l.chen@ucla.edu}
%\homepage[]{Your web page}
%\thanks{}
\author{Ata Mahjoubfar}
\affiliation{Department of Electrical Engineering, University of California, Los Angeles, California 90095, USA}
\affiliation{California NanoSystems Institute, Los Angeles, California 90095, USA}
\author{Li-Chia Tai}
\affiliation{California NanoSystems Institute, Los Angeles, California 90095, USA}
\author{Ian K. Blaby}
\affiliation{Biological, Environmental and Climate Sciences, Brookhaven National Laboratory, NY 11973, USA}
\author{Allen Huang}
\affiliation{Department of Electrical Engineering, University of California, Los Angeles, California 90095, USA}
\author{Kayvan R. Niazi}
\affiliation{California NanoSystems Institute, Los Angeles, California 90095, USA}
\affiliation{NantWorks, LLC, Culver City, California 90232, USA}
\affiliation{Department of Bioengineering, University of California, Los Angeles, California 90095, USA}
\author{Bahram Jalali}
\affiliation{Department of Electrical Engineering, University of California, Los Angeles, California 90095, USA}
\affiliation{California NanoSystems Institute, Los Angeles, California 90095, USA}
\affiliation{Department of Bioengineering, University of California, Los Angeles, California 90095, USA}
\affiliation{Department of Surgery, David Geffen School of Medicine, University of California, Los Angeles, California 90095, USA}

%Collaboration name if desired (requires use of superscriptaddress
%option in \documentclass). \noaffiliation is required (may also be
%used with the \author command).
%\collaboration can be followed by \email, \homepage, \thanks as well.
%\collaboration{}
%\noaffiliation

\date{\today}

\begin{abstract}
Label-free cell analysis is essential to personalized genomics, cancer diagnostics, and drug development as it avoids adverse effects of staining reagents on cellular viability and cell signaling. However, currently available label-free cell assays mostly rely only on a single feature, namely on cell size or dielectric constant. Given the diversity of cell populations, a single feature may not provide sufficient differentiation. Also, the sample size that can be analyzed by them is limited due to low throughput. Here, we integrate high-throughput quantitative imaging enabled by the photonic time stretch with feature extraction and deep learning, achieving record high accuracy in label-free cell classification. Our system captures quantitative optical phase and intensity images in a flow-through microscope and extracts multiple biophysical features of individual cells, such as morphological attributes, optical loss characteristics, and protein concentration indicators. These biophysical measurements form a hyperdimensional feature space in which supervised deep learning is performed for cell classification. We demonstrate and compare various learning algorithms including support vector machine, logistic regression, deep neural network and naive Bayes. A novel deep learning pipeline is introduced by adopting global optimization of receiver operating characteristics. As a validation of the enhanced classification sensitivity and specificity of our method, we show classification of \textit{OT-II} white blood T-cells against \textit{SW-480} colon cancer cells. Additionally, we show classification of lipid accumulating algal strains for biofuel production. This system opens up a new path to data-driven phenotypic diagnosis and better understanding of the heterogeneous gene expressions in cells.
\end{abstract}

% insert suggested PACS numbers in braces on next line
%\pacs{}
% insert suggested keywords - APS authors don't need to do this
\keywords{Label-free cell analysis, Quantitative phase imaging, Time stretch, Dispersive Fourier transform, Machine learning, Deep learning, SVM, Naive Bayes, Neural network, Feature extraction, Principal component analysis}

%\maketitle must follow title, authors, abstract, \pacs, and \keywords
\maketitle

% body of paper here - Use proper section commands
% References should be done using the \cite, \ref, and \label commands

Deep learning refers to methods for extraction of patterns and knowledge from rich multidimenstional data sets. While it is extensively used for image recognition and speech processing, its application to label-free classification of cells has not been exploited. Flow cytometry is a powerful tool for large-scale cell analysis due to its ability to measure anisotropic elastic light scattering of millions of individual cells as well as emission of fluorescent labels conjugated to cells \cite{shapiro2005practical,watson2004introduction}. However each cell is represented with single values per detection channels (forward scatter, side scatter, and emission bands), requiring labeling with specific biomarkers for acceptable classification accuracy \cite{shapiro2005practical,perfetto2004seventeen}. Imaging flow cytometry \cite{basiji2007cellular,basiji2001imaging} on the other hand captures images of cells revealing significantly more information about the cells. For example, it can distinguish clusters and debris that would otherwise result in false positive identification in a conventional flow cytometer based on light scattering \cite{carpenter2006cellprofiler}.

In addition to classification accuracy, another critical specification of a flow cytometer is its throughput. Indeed high throughput, typically 100,000 cells per second, is needed to screen a large enough cell population to find rare abnormal cells that are indicative of early stage diseases. However there is a fundamental trade-off between throughput and accuracy in any measurement system \cite{razavi1995principles,mahjoubfar2013optically}. Additionally, imaging flow cytometers face a throughput limit imposed by the speed of the CCD or the CMOS cameras, a number that is approximately 2000 cells/s for present systems \cite{goda2012high}. Higher flow rates lead to blurred cell images due to the finite camera shutter speed. Many applications of flow analyzers such as cancer diagnostics, drug discovery, biofuel development, and emulsion characterization require classification of large sample sizes with a high-degree of statistical accuracy \cite{zanella2010high}. This has fueled research into alternative optical diagnostic techniques for characterization of cells and particles in flow.

Recently, our group has developed a label-free imaging flow-cytometry technique based on coherent optical implementation of the photonic time stretch concept \cite{mahjoubfar2013label}. This instrument overcomes the trade-off between sensitivity and speed by using Amplified Time-stretch Dispersive Fourier Transform \cite{goda2013dispersive, solli2009optical, goda2009theory}. In time stretched imaging \cite{goda2009serial}, the object's spatial information is encoded in a spectrum of laser pulses within a pulse duration of sub-nanoseconds (Fig.~\ref{fig:Setup}). Each pulse representing one frame of the camera is then stretched in time so that it can be digitized in real-time by an electronic analog-to-digital converter (ADC). The ultra-fast pulse illumination freezes the motion of high-speed cells or particles in flow to achieve blur-free imaging. Detection sensitivity is challenged by the low number of photons collected during the ultra-short shutter time (optical pulse width) and the drop in the peak optical power resulting from the time stretch. These issues are solved in time stretch imaging by implementing a low noise-figure Raman amplifier within the dispersive device that performs time stretching \cite{goda2009serial,mahjoubfar2013label,mahjoubfar2013optically}. Moreover, warped stretch transform \cite{jalali2015tailoring} can be used in time stretch imaging to achieve optical image compression and nonuniform spatial resolution over the field-of-view \cite{chen2015optical}. In the coherent version of the instrument, the time stretch imaging is combined with spectral interferometry to measure quantitative phase and intensity images in real-time and at high throughput \cite{mahjoubfar2014label}. Integrated with a microfluidic channel, coherent time stretch imaging system in this work measures both quantitative optical phase shift and loss of individual cells as a high-speed imaging flow cytometer, capturing 36 million images per second in flow rates as high as 10 meters per second, reaching up to 100,000 cells per second throughput.

On another note, surface markers used to label cells, such as EpCAM \cite{gires2009abundance}, are indispensable tools for cell classification. However, they are unavailable in some applications; for example, melanoma or pancreatic circulating tumor cells (CTCs) as well as some cancer stem cells are EpCAM-negative and will escape EpCAM-based detection platforms \cite{kling2012beyond}. Furthermore, large-population cell sorting opens the doors to downstream operations, where the negative impacts of labels on cellular behavior and viability are often unacceptable \cite{boddington2011labeling}. Cell labels may cause activating/inhibitory signal transduction, altering the behavior of the desired cellular subtypes, potentially leading to errors in downstream analysis, such as DNA sequencing and subpopulation regrowth. In this way, quantitative phase imaging (QPI) methods \cite{ikeda2005hilbert,popescu2011quantitative,pham2013real} that categorize unlabeled living cells with high accuracy are needed. Coherent time stretch imaging is a method that enables quantitative phase imaging at ultrahigh throughput for non-invasive label-free screening of large number of cells.

In this work, the information of quantitative optical loss and phase images are fused into expert designed features, leading to a record label-free classification accuracy when combined with deep learning. Image mining techniques are applied, for the first time, to time stretch QPI to measure biophysical attributes including protein concentration, optical loss, and morphological features of single cells at an ultrahigh flow rate and in a label-free fashion. These attributes differ widely \cite{feinerman2008variability, sigal2006variability, friebel1999optical, vona2000isolation} among cells and their variations reflect important information of genotypes and physiological stimuli \cite{spencer2009non}. The multiplexed biophysical features thus lead to information-rich hyper-dimensional representation of the cells for label-free classification with high statistical precision. 

We further improved the accuracy, repeatability, and the balance between sensitivity and specificity of our label-free cell classification by a novel machine learning pipeline, which harnesses the advantages of multivariate supervised learning, as well as unique training by evolutionary global optimization of receiver operating characteristics (ROC) \cite{bradley1997use, powers2011evaluation, huang2005using}. To demonstrate sensitivity, specificity, and accuracy of multi-feature label-free flow cytometry using our technique, we classified (1) \textit{OT-II} hybridoma T-lymphocytes and \textit{SW-480} colon cancer epithelial cells, and (2) Chlamydomonas reinhardtii algal cells (herein referred to as Chlamydomonas) based on their lipid content, which is related to the yield in biofuel production. Our preliminary results show that compared to classification by individual biophysical parameters, our label-free hyperdimensional technique improves the detection accuracy from 78.1\% to 95.5\% [], or in other words, reduces the classification inaccuracy by about five times. 

\section{Results}
\subsection{Time Stretch Quantitative Phase Imaging}

The application of time stretch quantitative phase imaging (TS-QPI) to imaging flow cytometry has been recently demonstrated in our group \cite{mahjoubfar2013label}. Broadband optical pulses from a mode-locked laser were firstly conditioned in fiber optics and then spatially dispersed in free-space optics with a pair of reflection diffraction gratings creating 1-D ``rainbow flashes'' (Fig.~\ref{fig:Setup}). Each one of rainbow flashes was composed of all the wavelength components distributed laterally over the field of view. These flashes illuminated the target as in traditional photography, but in addition, rainbow flashes targeted different spatial points with distinct colors of light, resulting in space-to-spectrum encoding. Rainbow pulses were then split into the two arms of a Michelson interferometer. Different wavelength components of the rainbow flash traveled parallel to each other but individually focused on the mirror in the reference arm or on the reflective substrate of a microfluidic device in the sample arm. In the sample arm, the cells in the microfluidic channel were hydrodynamically focused \cite{knight1998hydrodynamic,lee2006hydrodynamic} into the rainbow's field of view and flowed perpendicular to the rainbow flash. Reflected pulses from the microfluidic device and the reference arm were recombined and coupled back into the fiber, optically amplified and linearly chirped through Raman-amplified time-stretch dispersive Fourier transform (TS-DFT) system. An amplified time-stretch system that utilizes a low-noise distributed Raman amplifier within dispersive fiber with a net optical gain of approximately 15 dB enables high-sensitivity detection at high speeds. An ultrafast single-pixel photodetector transformed instantaneous optical power into an electrical signal and subsequently, an analog-to-digital converter (ADC) samples and quantizes the signal. Acquired data are passed down to processing stages for big data analytics. The interference between time-shifted linearly chirped pulses create a beat (fringe) frequency, which can be adjusted via the interferometer arm length mismatch. Details of the demonstration system can be found in Section \ref{scn:Methods}A.

\begin{figure*}
\includegraphics[scale=0.2]{FigureSetup.jpg}
\caption{\label{fig:Setup} Time stretch quantitative phase imaging (TS-QPI) and analytics system; A mode-locked laser followed by a nonlinear fiber, an erbium doped fiber amplifier (EDFA), and a wavelength-division multiplexing (WDM) filter generate and shape a train of broadband optical pulses. Box 1: The pulse train is spatially dispersed into a train of rainbow flashes illuminating the target as line scans. The spatial features of the target are encoded into the spectrum of the broadband optical pulses, each representing a one-dimensional frame. The ultra-short optical pulse illumination freezes the motion of cells during high speed flow to achieve blur-free imaging with a throughput of 100,000 cells/s. The phase shift and intensity loss at each location within the field of view are embedded into the spectral interference patterns using a Michelson interferometer. Box 2: The interferogram pulses were then stretched in time so that spatial information could be mapped into time through time-stretch dispersive Fourier transform (TS-DFT), and then captured by a single pixel photodetector and an analog-to-digital converter (ADC). The loss of sensitivity at high shutter speed is compensated by stimulated Raman amplification during time stretch. Box 3: (a) Pulse synchronization; the time-domain signal carrying serially captured rainbow pulses is transformed into a series of one-dimensional spatial maps, which are used for forming line images. (b) The biomass density of a cell leads to a spatially varying optical phase shift. When a rainbow flash passes through the cells, the changes in refractive index at different locations will cause phase walk-off at interrogation wavelengths. Hilbert transformation and phase unwrapping are used to extract the spatial phase shift. (c) Decoding the phase shift in each pulse at each wavelength and remapping it into a pixel reveals the protein concentration distribution within cells. The optical loss induced by the cells, embedded in the pulse intensity variations, is obtained from the amplitude of the slowly varying envelope of the spectral interferograms. Thus, quantitative optical phase shift and intensity loss images are captured simultaneously. Both images are calibrated based on the regions where the cells are absent. Cell features describing morphology, granularity, biomass, etc are extracted from the images. (d) These biophysical features are used in a machine learning algorithm for high-accuracy label-free classification of the cells.}
\end{figure*}

\begin{figure*}
\includegraphics[scale=0.2]{Figure2DImage.jpg}
\caption{\label{fig:2DImage} Quantitative optical phase and loss images of \textit{OT-II} (blue) and \textit{SW-480} (green box) cells; The optical loss images of the cells are affected by the attenuation of multiplexed wavelength components passing through the cells. The attenuation itself is governed by the absorption of the light in cells as well as the scattering from the surface of the cells and from the internal cell organelles. The optical loss image is derived from the low frequency component of the pulse interferograms. The optical phase image is extracted from the analytic form of the high frequency component of the pulse interferograms using Hilbert Transformation, followed by a phase unwrapping algorithm. Details of these derivations can be found in Section \ref{scn:Methods}. Also, supplementary Videos 1 and 2 show measurements of cell-induced optical path length difference by TS-QPI at four different points along the rainbow for \textit{OT-II} and \textit{SW-480}, respectively.}
\end{figure*}

The photodetected time-stretched pulses, each representing one line scan, are converted to analytic signals using Hilbert transformation \cite{king2009hilbert} and the intensity and phase components are extracted. The phase component is a fast oscillating fringe (carrier frequency), caused by the interference of the linearly chirped pulses arriving from the reference and signal arms in the Michelson interferometer. Acting as a radio-frequency (RF) carrier whose frequency is set by the user adjusted arm length mismatch, the fringe frequency is modulated when the optical path length in the sample arm is changed by the arrival of a cell. This frequency shift and the accompanying phase change are used to measure the optical path length of the cell (see Section \ref{scn:Methods}B). Since the phase varies over a wide range (much larger than $2 \pi$ radians), an unwrapping algorithm is used to obtain the continuous phase profile. The phase profile contains an increasing term with time, corresponding to the fringe (beat) frequency and the phase shift induced by the cell. By eliminating the background phase component, the cell-induced phase shift is extracted. The second component in the waveform is a lower frequency envelope corresponding to temporal shape of the optical pulse. The amplitude of this envelope provides information about optical loss caused by transparency, surface roughness, and inner organelle complexity (Section \ref{scn:Methods}C). 

\subsection{Feature Extraction}

The decomposed components of sequential line scans form pairs of spatial maps, namely, optical phase and loss images (see Section \ref{scn:Methods}D). These images are used to obtain biophysical fingerprints of the cells \cite{mahjoubfar2013optically, driscoll2012automated}. With domain expertise, raw images are fused and transformed into a suitable set of biophysical features, listed in Table~\ref{tbl:Features}, which the deep learning model further converts into learned features for improved classification. 

The feature extraction operates on optical phase and loss images simultaneously, including object detection, segmentation, and feature measurement, as well as clump identification, noise suppression, etc. As an example of the expert designed features, the average refractive index, used as a measure of protein concentration \cite{barer1953refractometry}, is obtained by dividing the integral of the optical path length by the cell volume. Since cells in suspension relax to a spherical shape (due to surface tension) \cite{revel1974adhesion,whur1977substrate}, an independent measure of cell diameter can be obtained from its lateral dimension for volume estimation. 

In feature extraction, one of the most important advantages of optical loss and phase fusion, is its robustness and insensitivity to axial defocusing \cite{spadinger1990effect} caused by the limited depth-of-focus of the objective lens and variations of the cell alignment in microfluidic channel. Diffracted photons have little chance to be influential in phase images. This makes the size measurements in optical phase images relatively accurate and consistent, more suitable than direct size measurements in optical loss images for extraction of scattering and absorption features. Among different features, size measurement is particularly important as it is used by itself in many technologies \cite{adams2008highly, nagrath2007isolation, vona2000isolation, gossett2010label}.  

The large data set captured by TS-QPI provides sufficient statistical characteristics for cell analysis based on biophysical features. Since cells from even the same line or tissue exhibit variations in size, structure, and protein expression levels \cite{kaern2005stochasticity, maheshri2007living, zangle2014live}, high accuracy classification can only be achieved by a model tolerant to these intrinsic variations 
On the other hand, the feature extractor must reflect the intricate and tangled characteristics caused by extrinsic variations, eg. drug treatment \cite{spencer2009non}, cell cycles, rare cell types, labeling, and transcription rate \cite{johnston2012mitochondrial}. 

\begin{table*}[hb!]
\caption{\label{tbl:Features} List of extracted features}
\begin{tabular}{|p{0.13\textwidth}|p{0.6\textwidth}|p{0.11\textwidth}|}
\hline
Feature Name	 &Description	 &Category\\ \hline
Diameter-RB	 &Diameter along the interrogation rainbow. It is insensitive to flow rate fluctuation. For higher accuracy, it is calibrated by the spatial nonuniform distribution of rainbow wavelengths. 	 &Morphology\\ \hline
Diameter-FL	 &Diameter along the flow direction. It is sensitive to flow rate fluctuation, but can be a candidate parameter for monitoring flow speed and channel condition.	 &Morphology\\ \hline
Tight Area	 &Total number of pixels in the segmented region in the phase image	 &Morphology\\ \hline
Perimeter	 &Total number of pixels around the boundary of each segmented region	 &Morphology\\ \hline
Circularity	 &$4\pi \text{Area} / \text{Perimeter}^2$  &Morphology\\ \hline
Major Axis 	 &Considering the cell as elliptical in lateral imaging plane, the length of the major axis of the ellipse with a normalized second central moment same as the cell.	 &Morphology\\ \hline
Orientation	 &Angle between the flow direction and the major axis of the cell elliptical shape	 &Morphology\\ \hline
Loose Area	 &Total number of pixels in the expanded segmented region for measurement of the pixel intensities	 &Morphology\\ \hline
Median Radius	 &The median distance of any pixel in the object to the closest pixel outside of the object.	 &Morphology\\ \hline
OPD-1	 &Integrated optical path length difference within the entire segmented area (cell), calibrated by the power distribution within different wavelength components of the incident laser pulses.	 &Optical Phase\\ \hline
OPD-2	 &Integrated optical path length difference within the entire segmented area (cell). In addition to the calibration of OPD-1, it is calibrated by the pulse-to-pulse fluctuations within a \SI{1}{\micro\second} detection window.	 &Optical Phase\\ \hline
Refractive index	 &The mean refractive index difference between the object and the surrounding liquid (buffer solution), which is calculated based on OPD-2 and size measurement (see detail in Section \ref{scn:Methods}). Refractive index difference for cells is proportional to their protein concentration.	 &Optical Phase\\ \hline
Absorption-1	 &Mean absorption coefficient within the entire segmented area (cell). It is calibrated by the power distribution within different wavelength components of the incident laser pulses and by the pulse-to-pulse fluctuations within a \SI{1}{\micro\second} detection window. This parameter corresponds to an absorption-dominant model for the cell.	 &Optical Loss\\ \hline
Absorption-2	 &Mean absolute absorption coefficient within the entire segmented area (cell). It is calibrated by the power distribution within different wavelength components of the incident laser pulses and by the pulse-to-pulse fluctuations within a \SI{1}{\micro\second} detection window. This parameter corresponds to an absorption-dominant model for the cell.	 &Optical Loss\\ \hline
Scattering-1	 &Mean optical loss within the entire segmented area (cell). It is calibrated by the power distribution within different wavelength components of the incident laser pulses and by the pulse-to-pulse fluctuations within a \SI{1}{\micro\second} detection window. This parameter corresponds to a scattering-dominant model for the cell.	 &Optical Loss\\ \hline
Scattering-2	 &Mean absolute optical loss within the entire segmented area (cell). It is calibrated by the power distribution within different wavelength components of the incident laser pulses and by the pulse-to-pulse fluctuations within a \SI{1}{\micro\second} detection window. This parameter corresponds to a scattering-dominant model for the cell.	 &Optical Loss\\
\hline
\end{tabular}
\end{table*}
% \begin{table}%[H] add [H] placement to break table across pages
% \caption{\label{}}
% \begin{ruledtabular}
% \begin{tabular}{}
% Lines of table here ending with \\
% \end{tabular}
% \end{ruledtabular}
% \end{table}

A total of 16 features are chosen among the features extracted from fusion of optical phase and loss images of each cell. Features that are highly correlated do not provide unique information. Pairwise correlation matrix among these features is shown as a heat map in Fig.~\ref{fig:FeaturesCorrRank}a. Diagonal elements of the matrix are correlation of each feature with itself, i.e. the autocorrelation. The subset of the features in Box 1 shows high correlation among morphological features. Also, the subset features in Box 2 and 3 are correlated as they are mainly related to optical phase shift and optical loss, respectively. 

As a representation of our biophysical features in classification, Fig.~\ref{fig:FeaturesCorrRank}b shows classification accuracy based on each single feature arranged in descending order. The features are color coded into three categories: morphology, optical phase, and optical loss, to describe the main type of information provided by each. The figure provides valuable insight into the relative importance of each category of cell features and suggests that morphological features carries the most information about cells, but at the same time, significant additional information is contained in optical phase and loss measurements. 

\begin{figure*}
\includegraphics[scale=0.17]{FigureFeaturesCorrelationRank.jpg}
\caption{\label{fig:FeaturesCorrRank} (a) Pairwise correlation matrix visualized as a heat map. The map depicts the correlation between all major 16 features extracted from the quantitative images. Diagonal elements of the matrix represent correlation of each parameter with itself, i.e. the autocorrelation. The subsets in box 1, box 2, and box 3 show high correlation because they are mainly related to morphological, optical phase, and optical loss feature categories, respectively. (b) Ranking of biophysical features based on their accuracies in single-feature classification. Blue bars show performance of the morphological parameters, which includes diameter along the interrogation rainbow, diameter along the flow direction, tight cell area, loose cell area, perimeter, circularity, major axis length, orientation, and median radius. As expected, morphology contains most information, but other biophysical features can contribute to improved performance of label-free cell classification. Orange bars show optical phase shift features i.e. optical path length differences and refractive index difference. Green bars show optical loss features representing scattering and absorption by the cell. The best performed feature in these three categories are marked in red.}
\end{figure*}

\subsection{Machine Learning}

Neural networks are a flexible and powerful bioinspired learning model, which perform layers of nonlinear feature transformations, learned from the training data \cite{abu2012learning, bishop2006pattern, boddy1994neural}. The transformations morph the input data with weighted sums and nonlinear activation functions into feature spaces more suitable for classification. Shown in Fig.~\ref{fig:NeuralNet} is a feedforward neural network learning model used in our study. The figure also depicts one particular method of training via receiver operating characteristic (ROC). 

One learning algorithm introduced here maximizes the area under ROC curve (AUC), which is a global indicator of the classifier performance on the entire training dataset. The global training of the neural network, although computationally costly, results in a classifier more robust, repeatable, and insensitive to imbalance among classes. For the purpose of end-to-end supervised learning with AUC whose gradient is not well-behaved, we employed the heuristic genetic algorithm (GA), which is resilient to discontinuities of the cost function and being trapped in local minima during optimization.

\begin{figure*}
\includegraphics[scale=0.08]{FigureNeuralNet.jpg}
\caption{\label{fig:NeuralNet} Machine learning pipeline. Information of quantitative optical phase and loss images are fused to extract multivariate biophysical features of each cell, which are fed into a fully-connected neural network. The neural network maps input features by a chain of weighted sum and nonlinear activation functions into learned feature space, convenient for classification. This deep neural network is globally trained via area under the curve (AUC) of the receiver operating characteristics (ROC). Each ROC curve corresponds to a set of weights for connections to an output node, generated by scanning the bias node value. The training process maximizes AUC, pushing the ROC curve toward the upper left corner, which means improved sensitivity and specificity in classification.}
\end{figure*}

The network is composed of multiple hidden layers, which automatically learn representations of the data at different levels of abstraction, and thus, is considered a form of deep learning \cite{lecun2015deep, schmidhuber2015deep}. Each layer performs a linear combination on its inputs from the previous layer and operates a nonlinear function on the weighted sums. The output of the node $j$ in layer $l+1$, denoted by $z_j^{(l+1)}$ is generated from inputs $x_1$, $x_2$, \ldots, $x_N$ as
\begin{equation}
\begin{split}
& z_j^{(l+1)} = \h(a_j^{(l+1)}) = \h(\sum_{i=0}^{N_l} \omega_{ji}^{(l)} x_i^{(l)})\\
& l=0,\cdots,L;\quad j=1,\cdots,N_{l+1}
\end{split}
\end{equation}
Here $a_j^{(l+1)}$ is the linear combination of inputs, and $\omega_{ji}^{(l)}$ are the weights of the linear combination. The summation runs over $N_l$, the total number of nodes in the layer $l$, and $L$ is the total number of hidden layers. The constant term in the summation, $x_0^{(l)}$, is the bias node in layer $l$. Some popular choices for the nonlinear activation function $\h(\cdot)$ include logistic sigmoid function $\h(a)=1/(1+\exp(-a))$, hyperbolic tangent function $\tanh(a)$, and commonly used in deep learning, rectified linear unit (ReLU) $\h(a) = \max(0,a)$. In our learning model, we use ReLU, which typically speeds up the supervised learning process of deep neural network by inducing sparsity and preventing gradient vanishing problem. 

For a trained classifier in hyperspace, receiver operating characteristics (ROC) curve describes the sensitivity and specificity of a classifier collection that includes nonlinear classifiers scaled in the direction of their normal vector field. In a deep learning network, this is equivalent to shifting the value of the bias node in the last hidden layer. ROC highlights the trade-off between sensitivity and specificity (Fig.~\ref{fig:NeuralNet}), and the area under ROC (AUC) provides a quantitative robust measure of classifier performance \cite{hanley1982meaning, ling2003auc, cortes2004auc, liu2008roc}. Choosing a large value for the bias node results in high sensitivity, however this sacrifices the specificity leading to large number of false positives. As a way to visualize the impact of the threshold on classification accuracy, a classifier that accurately separates the classes will have an ROC curve that approaches the upper left corner. Conversely, a random guess, corresponding to accuracy of 50\% in binary classification will have an ROC that is a diagonal line. The AUC parameter serves as an effective analysis metric for finding the best classifier collection and has been proven to be advantageous to the mean square error for evaluating learning algorithms \cite{verrelst1998use}. 

To prevent overfitting in our deep learning model, we added a regularization term to the AUC-based cost function. Our regularization term is defined as mean square of all the network weights, excluding the bias nodes. Therefore, the overall cost function, $\cost(\omega)$, that is minimized by the genetic algorithm is
\begin{equation}
\begin{split}
\cost(\omega) = & (1 - \AUC(\omega)) \\
& + \lambda \frac{\sum_{l=0}^{L} \sum_{j=1}^{N_{l+1}} \sum_{i=1}^{N_l} (\omega_{ji}^{(l)})^2}{\sum_{l=0}^{L} \sum_{j=1}^{N_{l+1}} \sum_{i=1}^{N_l} 1}
\end{split}
\end{equation}
where $\lambda$ is the regularization parameter, which controls the trade-off between overfitting (variance) and underfitting (bias).

\subsection{Demonstration in Classification of \textit{OT-II} and \textit{SW-480} Cells}

In contrast to single-feature approaches \cite{nagrath2007isolation, vona2000isolation, kling2012beyond}, our label-free cell classification enabled by TS-QPI and multivariate analysis, offers considerable improvements in detection sensitivity and accuracy for cancer diagnosis. To demonstrate the application in circulating tumor cell (CTC) detection, we used \textit{OT-II} hybridoma T cells as a model for normal white blood cells and \textit{SW-480} epithelial colon cancer cells. The features described in \ref{tbl:Features} were measured by our TS-QPI system for the aforementioned cells. Fig.~\ref{fig:OTSWScatter} shows three of these features in a three-dimensional (3D) scatter plot, attributed to size, protein concentration, and attenuation. The 2D projections on the three orthogonal planes are also shown. It is clear that additional dimensions improve distinguishment among different cell types compared to individual features.

A 5-fold cross-validation methodology is applied on the dataset to split data points into training, validation, and test subsets (details in supplementary material Section~\ref{S-scn:CrossValidation}). Fig.~\ref{fig:OTSWROC}a shows progress in label-free classification depicted by balance accuracy as the learning model evolves over GA generations. Blue curve show the classification balanced accuracy of the test dataset using all sixteen biophysical features extracted from the TS-QPI images. To highlight the improvement by hyperdimensional feature space of TS-QPI, we also show the balanced accuracy curves based on several single features: cell diameter for morphology, integral of cell's optical path difference for optical phase information, and cellular absorption for optical loss in near-infrared window. Although these three biophysical features individually perform the highest accuracy among morphology, optical phase, and optical loss groups respectively, as previously shown in Fig.~\ref{fig:FeaturesCorrRank}b, our multivariate deep learning classifier outperforms them. In addition, receiver operating characteristic (ROC) curves for each fold are generated based on the test subsets (Fig.~\ref{fig:OTSWROC}b), and reveal the superior sensitivity and specificity of multivariate classifier. Also, the small variations of the ROC curves among different folds show the consistency of the classification performance for different test datasets. To visualize the hyperspace decision boundary, \textit{OT-II} and \textit{SW-480} data points are shown in first and second principal components analysis (PCA) components (Fig.~\ref{fig:OTSWROC}c).

\begin{figure*}
\includegraphics[scale=0.08]{FigureOTSWROC.jpg}
\caption{\label{fig:OTSWROC} Classification of white blood cells (\textit{OT-II}) and cancer cells (\textit{SW-480}) by TS-QPI label-free features; (a) Training process of the neural network leads to improvement of classification accuracy over generations of genetic algorithm. In addition to multivariate analysis using all 16 biophysical features extracted from the TS-QPI quantitative images, we also show training process by three single features (red, green, and orange curves). These curves represent the best biophysical feature in each category: morphology (Diameter-RB in Table~\ref{tbl:Features}), optical phase (OPD-1 in Table~\ref{tbl:Features}), and optical loss (Absorption-2 in Table~\ref{tbl:Features}). Clearly, the final achievable accuracy by multivariate classification is considerably higher than that of single features. (b) For each case, we show 5 ROC curves for different test datasets. The gray diagonal line shows results of random guess classification. Multivariate analysis based on TS-QPI images (blue curves) shows significant improvement in classification sensitivity and specificity. The fact that the classifiers remain almost unchanged during the five iterations of cross validation shows consistency and robustness of the classifiers. (c) To visualize the decision boundary of multivariate classification, data points are depicted in the space of the first two PCA components.}
\end{figure*}

\begin{figure*}
\includegraphics[scale=0.2]{FigureOTSWScatter.jpg}
\caption{\label{fig:OTSWScatter} Three-dimensional scatter plot based on size, protein concentration, and attenuation of \textit{OT-II} and \textit{SW-480} cells measured by TS-QPI. The green and blue dots are two-dimensional (2-D) projections of cell data points on the planes containing only two of the biophysical features. The cell protein concentration corresponds to the mean refractive index difference of the cell (Refractive index feature in Table \ref{tbl:Features}). The attenuation is a feature describing the optical intensity loss caused by cell absorption (Absorption-1 feature in Table \ref{tbl:Features}). Comparison of 2-D scatter plots reveals that additional biophysical features (in this case mainly protein concentration) serve to classify the cell types more accurately.}
\end{figure*}

\subsection{Demonstration in Algae Lipid Content Classification}

Microalgae are considered one of the most promising feedstock for biofuels \cite{merchant2012tag}. The productivity of these photosynthetic microorganisms in converting carbon dioxide into useful carbon-rich lipids greatly exceeds that of agricultural crops. Worldwide, research and demonstration programs are being carried out to develop the technology needed to expand algal lipid production as a major industrial process. Selecting high-yield microalgae with fast growth factors are essential in biofuel production industry. Because algae differ greatly in size and structure, cell size alone provides insufficient information for cell classification. Here we show that adding optical phase and loss data, obtained by the phase contrast time stretch imaging flow cytometer, to size data enables algal cells to be distinguished on the basis of lipid content. 

To test our apparatus for its ability to separate algal cells with high and low-lipid content, we exploited the starch-null \textit{sta6} strain of Chlamydomonas. This strain is deleted for \textit{sta6} \cite{zabawinski2001starchless} (encoding the small subunit of ADP-glucose-pyrophosphorylase), and when nitrogen-deprived accumulates more lipid than wild-type \cite{work2010increased, li2010chlamydomonas, goodenough2014path, blaby2013systems}. Comparison of the two strains therefore provides an ideal setup to test our ability to distinguish lipid-content phenotypes.

Fig.~\ref{fig:AlgaeScatter}a shows the 3D scatter plot showing the three principle physical features for the two algae populations. Here, the optical loss category of the features play a dominant role in label-free classification. In Fig.~\ref{fig:AlgaeScatter}b, we show ROC curves for binary classification of these populations. Blue curves show the classifier performance using all 16 physical features extracted from the TS-QPI images. Red, green, and orange curves show the classifier decision made using only the three major biophysical features: diameter for morphology (Diameter-RB in Table~\ref{tbl:Features}), optical path length difference for optical phase (OPD-1 in Table~\ref{tbl:Features}), and absorption for optical loss (Absorption-2 in Table~\ref{tbl:Features}). Our multivariate deep learning using TS-QPI is far more accurate than individual biophysical characteristics for selection of algal strains.

\begin{figure*}
\includegraphics[scale=0.8]{FigureAlgae.jpg}
\caption{\label{fig:AlgaeScatter} Classification of algal cells (Chlamydomonas reinhardtii) based on their lipid content by TS-QPI. (a) Three-dimensional scatter plot based on size, protein concentration, and attenuation of the cells measured by TS-QPI, with 2D projections for every combination of two features. Inset: Conventional label-free flow cytometry using forward scattering and side scattering is not enough to distinguish the difference between high-lipid content and low-lipid content algal cells. TS-QPI is much more effective in separating the two algae populations. (b) ROC curves for binary classification of normal and lipid-rich algae species using ten-fold cross validation; blue curves show the classifier performance using all 16 biophysical features extracted from the TS-QPI quantitative images. Red, green, and orange curves show the classifier decision performance using only the best biophysical feature in each category: morphology (Diameter-RB in Table~\ref{tbl:Features}), optical phase (OPD-1 in Table~\ref{tbl:Features}), and optical loss (Absorption-2 in Table~\ref{tbl:Features}). The label-free selection of algal strains improves as more biophysical features are employed.}
\end{figure*}

\section{Discussion}

To show the effect of the training dataset size in the performance of the learning model, the learning curves for the training and test datasets of the tumor cell detection are analyzed (Fig.~\ref{fig:LearningCurve}a). The test learning curve shows that as the number of training data points increases, the test error reduces and the model performance improves. On the other hand, the training error contrastingly increases for a larger number of training examples because it is more difficult for the learning model to fit many training data points than a few. The discrepancy of the training and test errors is the generalization error of the learning model \cite{abu2012learning}. Notice that beyond N = 2500 the generalization error do not decrease, and the learning curves converge to their ultimate performances. In other words, N = 2500 training data points are required to accomplish target achievable performance for the deep learning model used here.

Multiple machine learning techniques for multivariate label-free cell classification are compared using our TS-QPI tumor cell detection dataset (Fig.~\ref{fig:LearningCurve}b). The mean accuracies of all learning models are beyond [] 80\%, reflecting the advantages of simultaneous hyperdimensional biophysical features that TS-QPI provides for label-free cell classification. Furthermore, the standard deviation of the balanced accuracy (shown with error bars) is the smallest for the regularized AUC-based deep learning model, which confirms its consistency and repeatability are the best among learning methods.

\begin{figure*}
\includegraphics[scale=0.1]{FigureLearningCurve.jpg}
\caption{\label{fig:LearningCurve} (a) The learning curves of the training and test datasets in the tumor cell detection. Larger number of training data points decreases the cross entropy of the test dataset, which means the classifier is performing more accurately. However, the trend is opposite for the training dataset because with a larger number of training data points fitting error accumulates. The discrepancy of the training and test errors, i.e. generalization error, decreases up to N = 2500, which is the necessary training data size for achieving final performance in our TS-QPI demonstration with deep learning neural network. (b) Comparison of multiple machine learning classification techniques based on the biophysical features extracted from the label-free cell images captured by TS-QPI. Our AUC-based deep learning model has both the highest accuracy and consistency against support vector machine (SVM) with Gaussian kernel, logistic regression, naive Bayes, and conventional deep neural network trained by cross entropy and backpropagation.}
\end{figure*}

\section{Conclusion}

Time-stretch quantitative phase imaging (TS-QPI) is capable of capturing images of flowing cells with minimal motion distortion at unprecedented rates of 100,000 cells/s. TS-QPI relies on spectral multiplexing to capture simultaneously both phase and intensity quantitative images in a single measurement, generating a wealth of information of each individual cell eliminating the need for labeling with undesirable biomarkers. Here, we summarized the information content of these images in a set of 16 features for each cell, and performed classification in the hyperdimensional space composed of these features. We demonstrated application of various learning algorithms including deep neural networks, logistic regression, naive Bayes, as well as a new traning method based on area under the ROC curve. The results from two experimental demonstrations, one on detection of cancerous cells among white blood cells, and another one on identification of lipid-rich algae, show that classification accuracy by using the TS-QPI hyperdimensional space is more than [] 17\% better than the conventional size-based techniques. Our system paves the way to cellular phenotypic analysis as well as data-driven diagnostics, and thus, is a valuable tool for high-throughput label-free cell screening in medical, biotechnological, and research applications.

\section{\label{scn:Methods} Methods}

\subsection{Time Stretch Quantitative Phase Imaging (TS-QPI) System}

Broadband optical pulses from a mode-locked laser (center wavelength = 1565 nm, repetition rate = 36.6 MHz, pulse width $\approx100$ fs) are broadened using a highly nonlinear fiber to approximately 100 nm bandwidth with a spectral range up to 1605 nm. These broadband pulses are then linearly chirped to nanosecond pulse width by a short dispersion compensating fiber (DCF) of 60 ps/nm, so that an erbium doped fiber amplifier (EDFA) can amplify them with minimal distortion. A coarse wavelength division multiplexer (WDM) filters the pulses from 1581 nm to 1601 nm, where the spectrum is reasonably flat. Therefore, the total bandwidth of the pulses interrogating the cells in our setup is less than 20 nm centered at 1591 nm, giving a negligible fractional bandwidth of 1.3\%. These filtered pulses then pass through an optical circulator and are coupled to free-space with a fiber collimator.

Free-space laser pulses were linearly polarized with quarter- and half-wave plates, and then spatially dispersed with a pair of reflection diffraction gratings, so that each wavelength component of the collimated beam was positioned at a different lateral point similar to a line flash rainbow. A beam reducer shrank the rainbow beam 6 times with a pair of 90 degree off-axis parabolic gold-coated mirrors with reflected focal lengths as 152.4 mm and 25.4 mm, respectively. Next, a 15 degree off-axis parabolic gold-coated mirror with 635 mm reflected focal length and a long working-distance objective lens with 0.4 numerical aperture further shrank the rainbow to about \SI{130}{\micro\meter} in width, i.e. field of view. Reflective optics with parabolic gold-coated mirrors is used in our experimental demonstration to minimize loss, aberration, and polarization sensitivity. The rainbow flashes were then split into the two arms of a Michelson interferometer by a beam splitter. In the sample arm, the rainbow pulses pass through the cells and are reflected by the reflective substrate of the microfluidic device. In the reference arm, a dielectric mirror reflected the rainbow with a length mismatch with the sample arm causing spectral interference fringes (Fig.~\ref{S-fig:OSA}a). Cells are hydrodynamically focused at the center of the channel flow at a velocity of 1.3 m/s. The reflected pulses from reference and sample arms were recombined at the beam splitter, compressed by the two diffraction gratings and coupled back into the fiber. These return pulses were spectrally encoded by the spatial information of the interrogation field of view. Then they were redirected by the optical circulator to a Raman-amplified time-stretch dispersive Fourier Transform (TS-DFT) system followed by a 10 Gb/s photodetector (Discovery Semiconductors DSC-402APD). An analog-to-digital converter (Tektronix DPO72004C) with a sampling rate of 50 GS/s and 20 GHz bandwidth is used to acquire the output signal of the photodetector, which is a series of spectral interferograms mapped into time (Fig.~\ref{S-fig:OSA}b). 

\subsection{Coherent Detection and Phase Extraction}

Unlike in conventional heterodyne detection, which uses a narrowband continuous-wave signal as the local oscillator or reference, the coherent detection in our time stretch system uses an unmodulated copy of the original optical input, which is a broadband optical pulse train \cite{buckley2013coherent, devore2014coherent}. 

Since the spectrum is mapped into space by diffraction gratings, the complex field at any specific spatial location within the field of view is a narrowband optical wave. As the envelope of the optical wave varies slowly in time compared to the period of the optical electromagnetic wave and the time mismatch between the reference arm and the sample arm, we employ slowly varying envelope approximation in our analysis. The complex envelope of the input electric field, $\tilde{E}_{in}(\omega, t_p)$, is split into two arms of the Michelson interferometer at the beam splitter. Here, $\omega$ is the optical frequency of the input signal, which corresponds to the spatial location $x$ being interrogated by the optical wave at this frequency (i.e. spectral encoding of the object image). $t_p$ specifies the time when each rainbow flash reaches the beam splitter, corresponding to the $p$-th incoming pulse. Note that $\tilde{E}_{in}(\omega,t_p)$ can be simplified as $\tilde{E}_{in}(\omega)$ when pulse shape is stable from pulse to pulse. The light split into the two arms of the Michelson interferometer can be expressed as
\begin{equation}
\begin{split}
&\text{Into the sample arm:} \\
&\quad \tilde{E}_s(\omega, t_p) = \sqrt{T_b} \tilde{E}_{in}(\omega, t_p)  \\
&\text{Into the reference arm:} \\
&\quad \tilde{E}_r(\omega, t_p) = i \sqrt{1-T_b} \tilde{E}_{in}(\omega, t_p)
\end{split}
\end{equation}
where $T_b$ is the power transmission ratio of the beam-splitter. Optical intensity in the sample arm will be altered by the absorption and scattering of imaged cells, as well as that of the microfluidic channel and buffer solution. Not only the electric field amplitude after passing through semitransparent objects will be modulated by the optical attenuation in the sample arm, but also the optical path length difference will lead to a phase shift, $\Delta\varphi_c(x,t_p)$, induced by refractive index change from the object along the interrogation beam. Thus, the complex fields of the light waves coming back to the beam splitter become
\begin{equation}
\begin{split}
&\text{From the sample arm:}\\
&\quad \tilde{E}_s(\omega, t_p) = T_s(\omega) T_c(x,t_p + t_d) \sqrt{T_b(\omega) R_m(\omega)} \tilde{E}_{in}(\omega, t_p) \\
&\quad\quad\quad \cdot \exp \Big\lbrace i\Big[-\frac{\omega}{c} \cdot 2(L+\Delta L) - \Delta\varphi_c(x,t_p)\Big] \Big\rbrace\\
&\text{From the reference arm:} \\
&\quad \tilde{E}_r(\omega, t_p) = i \sqrt{1-T_b(\omega)} \sqrt{R_m(\omega)} \tilde{E}_{in}(\omega, t_p) \\
&\quad\quad\quad \cdot \exp(- i \frac{\omega}{c}\cdot 2L)
\end{split}
\end{equation}
where $L$ is the length of reference arm, and $\Delta L$ is the arm length mismatch between two arms. $R_m(\omega)$ is the wavelength-dependent reflectance of the reflective substrate of the microfluidic channel and the dielectric mirror in the reference arm. $t_d$ is the time delay during which rainbow flash travels from the beam splitter to the sample cell, $t_d = \frac{L_0 + \Delta L}{c}$. $T_s(\omega)$ is power transmittance of the surrounding buffer solution and microfluidic channel, and $T_c(x,t_p + t_d)$ is spatial power transmittance of cells at location $x$ along the rainbow when being illuminated at time $t_p + t_d$. Both $\sqrt{T_s(\omega)}$ and $\sqrt{T_c(x,t_p + t_d)}$ affect the optical field twice as each rainbow flash passes through the cell twice. Since the $t_d$ is much smaller than the time scale of the envelope variations caused by the cell flow, we can approximate $T_c(x,t_p + t_d)$ to be $T_c(x,t_p)$ to synchronize with $\tilde{E}_{in}(\omega, t_p)$ without sacrificing accuracy.

The total electric field at each wavelength or optical frequency after two arms of the interferometer recombine at the beam splitter becomes
\begin{equation} \label{eq:totalElecField}
\begin{split}
\tilde{E}_o(\omega, t_p) &= T_s(\omega) T_c(x, t_p) T_b(\omega) \sqrt{R_m(\omega)} \tilde{E}_{in}(\omega, t_p) \\
&\quad \cdot \exp \Big\lbrace i[-\frac{\omega}{c} \cdot 2(L+\Delta L) - \Delta\varphi_c(x,t_p)] \Big\rbrace \\
&\quad - [1-T_b(\omega)] \sqrt{R_m(\omega)} \tilde{E}_{in}(\omega, t_p) \exp[- i \frac{\omega}{c}\cdot 2L] \\
&= \sqrt{R_m(\omega)} \tilde{E}_{in}(\omega, t_p) \exp(- i \frac{2\omega L}{c}) \\
&\quad \cdot \Big\lbrace T_s(\omega) T_c(x, t_p) T_b(\omega) exp \lbrace i[-2\frac{\omega}{c}\Delta L  \\
&\quad - \Delta\varphi_c(x,t_p)] \rbrace - [1-T_b(\omega)] \Big\rbrace.
\end{split}
\end{equation}
Based on the spectral encoding setup, we know spatial information has been encoded into spectrum,
\begin{align}
T_c(x,t_p) &\Rightarrow T_c(\omega,t_p)\\
\Delta\varphi_c(x,t_p) &\Rightarrow \Delta\varphi_c(\omega,t_p)
\end{align}
The intensity envelope then becomes
\begin{equation} \label{eq:totalIntensity}
\begin{split}
\tilde{I}_o(\omega, t_p) &\propto \tilde{E}_o^*(\omega, t_p) \tilde{E}_o(\omega, t_p) \\
&= \Big\lbrace [1-T_b(\omega)]^2 + T_b^2(\omega) T_c^2(\omega, t_p) T_s^2(\omega) \Big\rbrace R_m(\omega) \\
& \cdot \lvert\tilde{E}_{in}(\omega, t_p)\rvert^2 - 2 [1-T_b(\omega)] T_c(\omega, t_p) T_b(\omega) R_m(\omega) \\
& \cdot T_s(\omega) \lvert\tilde{E}_{in}(\omega, t_p)\rvert^2 cos[2\omega \Delta L / c + \Delta \varphi_c(\omega, t_p) ]
\end{split}
\end{equation}

During time stretch, each frequency component $\omega$, or wavelength $\lambda$ will be one-to-one mapped into time domain. We define the relative time delay of $\lambda$ compared to the central wavelength, $\lambda_c$, as $t_i$, which is usually called intrapulse time delay. Written in terms of $\lambda$, Eq.~\ref{eq:totalIntensity} can be simplified as
\begin{equation} \label{eq:totalIntensitylambda}
\tilde{I}_o(\lambda, t_p) \propto I_b (\lambda, t_p) + I_i (\lambda, t_p) cos[ \frac{4\pi\Delta L}{\lambda} + \Delta \varphi_c(\lambda, t_p) ]
\end{equation}
where $I_b (\lambda, t_p)$ is the background or baseband intensity envelope, and $I_i (\lambda, t_p)$ is the interference or intermediate intensity envelope:
\begin{align}
\begin{split} \label{eq:backgroundIntensity}
I_b (\lambda, t_p) &= \Big\lbrace [1-T_b(\lambda)]^2 + T_b^2(\lambda) T_c^2(\lambda, t_p) T_s^2(\lambda) \Big\rbrace \\
&\cdot R_m(\lambda) \lvert \tilde{E}_{in}(\lambda, t_p)\rvert^2
\end{split}\\
\begin{split} \label{eq:interferenceIntensity}
I_i (\lambda, t_p) &= - 2 [1-T_b(\lambda)] T_c(\lambda, t_p) T_b(\lambda) R_m(\lambda)\\
& \cdot T_s(\lambda) \lvert \tilde{E}_{in}(\lambda, t_p)\rvert^2
\end{split}
\end{align}

Linear time stretch maps frequency domain into time domain by
\begin{equation} \label{eq:DCF}
t_i = D (\lambda - \lambda_c) L_f.
\end{equation}
where $\lambda_c$ is the central wavelength and $L_f$ the length of the dispersive fiber. $D$ is the group velocity dispersion, that is, the temporal pulse spreading, $\Delta t_i$, per unit bandwidth, $\Delta\lambda$, per unit distance traveled. Thus the temporal samples of the energy flux absorbed at the photodetector are the intrapulse concatenation of spectral samples followed by interpulse concatenation of pulse waveforms:
\begin{equation}
\begin{split}
\tilde{I}_{PD}[t] = \quad\quad&\quad\\
\horcat\vercat &
\begin{pmatrix}
\tilde{I}_{o}(t_i^{(1)}, t_p^{(1)})&	\tilde{I}_{o}(t_i^{(1)}, t_p^{(2)})&	\cdots& \tilde{I}_{o}(t_i^{(1)}, t_p^{(m)})\\
\tilde{I}_{o}(t_i^{(2)}, t_p^{(1)})&	\tilde{I}_{o}(t_i^{(2)}, t_p^{(2)})&	\cdots& \tilde{I}_{o}(t_i^{(2)}, t_p^{(m)})\\
\vdots&									\vdots&									\ddots& \vdots								 \\
\tilde{I}_{o}(t_i^{(n)}, t_p^{(1)})&	\tilde{I}_{o}(t_i^{(n)}, t_p^{(2)})&	\cdots& \tilde{I}_{o}(t_i^{(n)}, t_p^{(m)})\\
\end{pmatrix} \\
\rightleftharpoons \horcat\vercat &
\begin{pmatrix}
\tilde{I}_{o}(\lambda_1, t_p^{(1)})  &\tilde{I}_{o}(\lambda_1, t_p^{(2)})   &\cdots  &\tilde{I}_{o}(\lambda_1, t_p^{(m)})\\
\tilde{I}_{o}(\lambda_2, t_p^{(1)})  &\tilde{I}_{o}(\lambda_2, t_p^{(2)})   &\cdots  &\tilde{I}_{o}(\lambda_2, t_p^{(m)})\\
\vdots						   		 &\vdots							    	&\ddots  &\vdots					 \\
\tilde{I}_{o}(\lambda_n, t_p^{(1)})  &\tilde{I}_{o}(\lambda_n, t_p^{(2)})   &\cdots  &\tilde{I}_{o}(\lambda_n, t_p^{(m)})\\
\end{pmatrix} \\
\end{split}
\end{equation}
where $\horcat$ and $\vercat$ mean horizontal and vertical concatenations, respectively. Each $\tilde{I}_{o}(t_i^{(n)}, t_p^{(m)})$ expresses the $n$th spectral (spatial) pixel at the $m$th pulse (line image). Applying Eq.~\ref{eq:DCF} to Eq.~\ref{eq:totalIntensitylambda},
\begin{equation}\label{eq:afterDCF}
\begin{split}
\tilde{I}_{o}(t_i^{(n)}, t_p^{(m)}) & \propto I_b (t_i^{(n)}, t_p^{(m)}) + I_i (t_i^{(n)}, t_p^{(m)}) \\
& \cdot cos\Bigg[ \frac{4\pi\Delta L \cdot D L_f}{t_i^{(n)} + D\lambda_c L_f} + \Delta \varphi_c(t_i^{(n)}, t_p^{(m)}) \Bigg]
\end{split}
\end{equation}

Therefore, the time stretched temporal waveform corresponding to each line scan image consists of two features \cite{mahjoubfar2014label}: One is $I_b (t_i^{(n)}, t_p^{(m)})$, a temporal envelope of the time-stretched optical pulse at baseband frequencies. The amplitude of this envelope corresponds to the temporal shape of the optical pulse and its deviations caused by the object transmission as in brightfield microscopy. It provides information about optical loss, i.e. light absorption and scattering caused by surface roughness, granularity, and inner cell organelle complexity. 

The second term in Eq.~\ref{eq:afterDCF} (with cosine component) is a fast oscillating fringe, caused by the spectral interference of the pulses multiplexed between the sample and the reference arms in the Michelson interferometer. This term can be separated by a bandpass filter, and its envelope can be derived by a nonlinear envelope detection technique. Here we used a moving minimum/maximum filter to extract the envelope. After normalization to the envelope, the cosine component 
\begin{equation}
I_c (t_i^{(n)}, t_p^{(m)}) = cos\Bigg[ \frac{4\pi\Delta L \cdot D L_f}{t_i^{(n)} + D\lambda_c L_f} + \Delta \varphi_c(t_i^{(n)}, t_p^{(m)}) \Bigg]
\end{equation}
is used for calculation of the object phase shift, $\Delta\varphi_c(x,t_p)$. The first term in cosine causes the interferogram fringe pattern. Since $t_i \ll D \lambda_c L_f$, it can be approximated as
\begin{equation}
\frac{4\pi D L_f \Delta L}{t_i + D \lambda_c L_f} \approxeq - f_i t_i + \varphi_{i0}
\end{equation}
where $\varphi_{i0}$ is an initial phase constant, $f_i$ is the fringe frequency:
\begin{equation}
f_i \approxeq \frac{4\pi \Delta L}{\lambda_c^2 D L_f}
\end{equation}
As seen in Fig.~\ref{S-fig:OSA}b, the fringe frequency, $f_i$, in our setup is about 4.7 GHz determined by the optical path length mismatch between the interferometer arms.

The instantaneous phase of $I_c(t_i^{(n)}, t_p^{(m)})$ can be readily retrieved from its analytic representation given by Hilbert transform, $\hilbert$:
\begin{equation} \label{eqn:angle}
\begin{split}
\angle I_c(t_i^{(n)}, t_p^{(m)}) &= \arg\Big[I_c(t_i^{(n)}, t_p^{(m)}) + j \cdot \hilbert \lbrace I_c(t_i^{(n)}, t_p^{(m)}) \rbrace\Big] \\
&=  \frac{4\pi D L_f\Delta L}{t_i^{(n)} + D\lambda_c L_f} + \Delta \varphi_c(t_i^{(n)}, t_p^{(m)})
\end{split}
\end{equation}
Here $\arg$ means the argument of a complex number. A one-dimensional phase unwrapping algorithm followed by background phase removal gives the object phase shift, 
\begin{equation}
\begin{split}
\Delta\varphi_c(t_i^{(n)}, t_p^{(m)}) = \unwrap \lbrace &\angle I_c(t_i^{(n)}, t_p^{(m)}) \\
&- \angle I_c(t_i^{(n)}, t_p^{(\text{empty})}) \rbrace
\end{split}
\end{equation}
where $t_p^{(\text{empty})}$ corresponds to an empty pulse when no cell is in the field of view, i.e. background phase. The unwrapping algorithm used in our processing acts when the absolute phase difference between two consecutive samples of the signal is greater than or equal to $\pi$ radians, and adds multiples of $2\pi$ to the following samples in order to bring the consecutive samples phase difference in the acceptable range of $-\pi$ to $\pi$. 

To perform combined quantitative phase and loss imaging, the phase derived by Hilbert transformation should be corrected to eliminate the artifacts caused by the intensity variations induced by the passing cells. Most cells of interest in clinical or industrial applications have a diameter 3--\SI{40}{\micro\meter}, when suspended in fluid. Given the field of view and the period of the interrogation rainbow pulses are \SI{130}{\micro\meter} and \SI{27}{\nano\second}, respectively, the time duration of the instantaneous intensity change induced by the single cells in each laser pulse is about 0.6--\SI{8.3}{\nano\second}, which will bring in frequency components up to about 1.6 GHz. Compared to the higher frequency components at 4.7 GHz corresponding to the interference fringes, the frequency of intensity variations is small ($<$1.6 GHz), and in this scenario, our method remains robust to separate the two electrical spectral components for optical loss and phase.

\subsection{Cell Transmittance Extraction} \label{scn:transmittance}
One of the greatest advantage of TS-QPI is its ability to extract the cell transmittance, $T_s(\lambda)$, without prior knowledge of the transmittance of the solution, $T_s(\lambda)$, that of the beam-splitter, $T_b(\lambda)$, and the reflectance of substrate of the microfluidic channel, $R_m(\lambda)$. During measurements when there is no cell in the field of view (empty frames), Eq. \ref{eq:interferenceIntensity} becomes
\begin{align}
\begin{split} \label{eq:interferenceIntensityEmpty}
I_i (\lambda, t_p^{(empty)}) &= - 2 [1-T_b(\lambda)] T_b(\lambda) R_m(\lambda)\\
& \cdot T_s(\lambda) \lvert \tilde{E}_{in}(\lambda, t_p^{(empty)})\rvert^2
\end{split}
\end{align}

In addition, the signal from only the reference arm can be recorded by blocking the sample arm:
\begin{equation} \label{eq:referenceIntensity}
I_r (\lambda, t_p) = [1-T_b(\lambda)]^2 R_m(\lambda) \lvert\tilde{E}_{in}(\lambda, t_p)\rvert^2
\end{equation}
Combining Eq. \ref{eq:backgroundIntensity}, \ref{eq:interferenceIntensityEmpty}, and \ref{eq:referenceIntensity}, and assuming that the input electric field pulse shape, $\lvert \tilde{E}_{in}(\lambda, t_p) \rvert$, is invariant to $t_p$, the cell transmittance can be derived as
\begin{equation}
T_c(\lambda) = \frac{-2\sqrt{I_r (\lambda, t_p) \cdot (I_b (\lambda, t_p) - I_r (\lambda, t_p))}}{I_i (\lambda, t_p^{(empty)})}
\end{equation}
Please note that the values of $I_r (\lambda, t_p)$, $I_b (\lambda, t_p)$, and $I_i (\lambda, t_p^{(empty)})$ are directly measured by TS-QPI, and no prior knowledge of $T_b(\lambda)$, $T_s(\lambda)$, $R_m(\lambda)$, and $\lvert \tilde{E}_{in}(\lambda, t_p)\rvert$ is needed to calculate the cell transmittance.

\subsection{Image Reconstruction}

We reconstruct both quantitative brightfield and phase-contrast images simultaneously from single-shot frequency-multiplexed interferometric measurements. The envelope and phase of the time-domain signal $\tilde{I}_o (t_i^{(n)}, t_p^{(m)})$ was firstly mapped into series of spatial information $\tilde{I}_o(x^{(n)},t_p^{(m)})$, forming a line scanning brightfield image and phase contrast image, illuminated by the optical pulse at time $t_p$. This is because within each optical pulse, the spatial information is mapped one-to-one into spectral domain, $x^{(n)} \rightarrow \lambda_n$, and spectrum is stretch in time, $\lambda_n \rightarrow t_i^{(n)}$, where $t_i^{(n)}$ is the relative group delay time of each frequency component within a pulse with respect to the central wavelength. These line-scan images based on $\tilde{I}_o(x,t_p^{(1)})$, $\tilde{I}_o (x, t_p^{(2)})$, $\tilde{I}_o(x,t_p^{(3)})$, \ldots were then cascaded into a two dimensional image corresponding to $\tilde{I}_o(x,y)$, where the second dimension $y$ is the spatial mapping of time lapse based on object flow speed. 

The optical path length difference image can be calculated by the phase shift line scans as
\begin{equation}
OPD(x^{(n)},y^{(m)}) = \frac{\lambda(t_i^{(n)},t_p^{(m)})}{2\pi} \Delta\varphi_c(t_i^{(n)},t_p^{(m)})
\end{equation}
On the other hand, if the axial thickness of the cell at reconstructed image pixel $(x,y)$ is $d(x,y)$,
\begin{equation} \label{eq:OPD}
OPD(x,y) = 2 [n_{cell}(x,y) - n_{solution}(x,y)] \cdot d(x,y)
\end{equation}
in which $n_{cell}$ and $n_{solution}$ are the refractive indices of the cell and the surrounding buffer solution, respectively. The factor 2 is to account for the fact that each wavelength component passes the cell twice in Michelson interferometer. 

If we integrate Eq.~\ref{eq:OPD} over the area of the cell, we can derive an average refractive index contrast for the cell, which corresponds to the average protein concentration of the cell:
\begin{equation}
\overline{\Delta n_{cell}} = \overline{n_{cell} - n_{solution}} = \frac{\iint_{cell} OPD(x,y) dx dy}{2 V}
\end{equation}
where $\iint_{cell} t(x,y) dx dy$ is the volume of the cell obtained from its diameter, $d$, as $V \approx \pi d^3/6$. 

The unit net change of intensity envelope variations induced by the cell is obtained from the amplitude of the slowly varying envelope feature of the interferogram as
\begin{equation}
\Delta I_b(\lambda, t_p) = \frac{T_b^2(\lambda) T_s^2(\lambda) (1 - T_c^2(\lambda, t_p))}{[1-T_b(\lambda)]^2 + T_b^2(\lambda) T_s^2(\lambda)}
\end{equation}
It gives the temporal and spatial information of the combined effects from absorption and scattering:
\begin{equation}
I_{loss}(x^{(n)}, y^{(m)}) = \Delta I_b(\lambda_n, t_p^{(m)})
\end{equation}

\subsection{Big Data Analytics Pipeline}

The high-content image analysis and cell screening pipeline is implemented by combining multiple informatics tools, namely CellProfiler for image processing \cite{carpenter2006cellprofiler,kamentsky2011improved}, MySQL/MangoDB for database, Matlab for machine learning, and Javascript for interactive visualization. First of all, image noise reduction and smoothing have been performed, which can remove artifacts that are smaller than optical resolution limit. For object segmentation, we use the Otsu's thresholding method. Once objects are identified in the image, morphology of each single cell can be described by area, diameter, uniformity, aspect ratio, perimeter, number of surrounding clumped cells, etc.

The capability to identify clumped cells from single large cells greatly reduces the misclassification rate in imaging flow cytometry compared to traditional flow cytometry. Intensity peaks of pixel brightness within each object are used to distinguish clumped objects. The object centers are defined as local intensity maxima in the smoothed image. Retaining outlines of the identified objects helps validate and visualize the algorithm. In the next step, we discard the objects touching the borders of the image, i.e., the edges of the field of view and data acquisition time window. However, the chance of cells showing up at the edges is very low due to hydrodynamic focusing. We are also capable of excluding dust, noise, and debris by neglecting the objects that are too small or their aspect ratio is too extreme to be a cell.

To calibrate the imaging system and image processing pipelines for size measurement, \SI{5}{\micro\meter} polystyrene beads (from Polysciences, Inc.) with NIST traceable particle size standards were analyzed. Size measurement of the polystyrene beads had a distribution with \SI{5.06}{\micro\meter} expected mean and \SI{0.5}{\micro\meter} standard deviation. The broadened standard deviation was within the range of optical resolution limit and was caused mainly by performing object recognition on resolution limited images. Due to limited optical resolution of the setup, the edges of bead or cell are blurred, generating distribution of point spread functions in optical phase and loss images outside of the cell boundaries. In order to maximize the accuracy in morphological, phase, and loss measurements, after object segmentation we expanded the object boundaries by \SI{2.5}{\micro\meter} (optical resolution of the setup measured by knife-edge method), which serve as loose boundaries, indicating the area within which the pixel intensities are measured and integrated in phase and loss images.

\subsection{Data Cleaning}

Data cleaning includes two steps. Firstly, Hotelling's T-squared distribution is calculated and the top 2\% of the extreme data was set as outliers due to experimental or object recognition errors. Secondly, debris discrimination is performed; any data point with negative phase shift was considered as either air bubble, flow turbulence, or object recognition errors.

% Put \label in argument of \section for cross-referencing
%\section{\label{}}
%\section{}
%\subsection{}

% If in two-column mode, this environment will change to single-column
% format so that long equations can be displayed. Use
% sparingly.
%\begin{widetext}
% put long equation here
%\end{widetext}

% figures should be put into the text as floats.
% Use the graphics or graphicx packages (distributed with LaTeX2e)
% and the \includegraphics macro defined in those packages.
% See the LaTeX Graphics Companion by Michel Goosens, Sebastian Rahtz,
% and Frank Mittelbach for instance.
%
% Here is an example of the general form of a figure:
% Fill in the caption in the braces of the \caption{} command. Put the label
% that you will use with \ref{} command in the braces of the \label{} command.
% Use the figure* environment if the figure should span across the
% entire page. There is no need to do explicit centering.

% \begin{figure*}
% \includegraphics{}%
% \caption{\label{}}
% \end{figure*}

% Surround figure environment with turnpage environment for landscape
% figure
% \begin{turnpage}
% \begin{figure*}
% \includegraphics{}%
% \caption{\label{}}
% \end{figure*}
% \end{turnpage}

% tables should appear as floats within the text
%
% Here is an example of the general form of a table:
% Fill in the caption in the braces of the \caption{} command. Put the label
% that you will use with \ref{} command in the braces of the \label{} command.
% Insert the column specifiers (l, r, c, d, etc.) in the empty braces of the
% \begin{tabular}{} command.
% The ruledtabular enviroment adds doubled rules to table and sets a
% reasonable default table settings.
% Use the table* environment to get a full-width table in two-column
% Add \usepackage{longtable} and the longtable (or longtable*}
% environment for nicely formatted long tables. Or use the the [H]
% placement option to break a long table (with less control than 
% in longtable).
% \begin{table}%[H] add [H] placement to break table across pages
% \caption{\label{}}
% \begin{ruledtabular}
% \begin{tabular}{}
% Lines of table here ending with \\
% \end{tabular}
% \end{ruledtabular}
% \end{table}

% Surround table environment with turnpage environment for landscape
% table
% \begin{turnpage}
% \begin{table}
% \caption{\label{}}
% \begin{ruledtabular}
% \begin{tabular}{}
% \end{tabular}
% \end{ruledtabular}
% \end{table}
% \end{turnpage}

% Specify following sections are appendices. Use \appendix* if there
% only one appendix.
%\appendix
%\section{}

% Create the reference section using BibTeX:
%merlin.mbs apsrev4-1.bst 2010-07-25 4.21a (PWD, AO, DPC) hacked
%Control: key (0)
%Control: author (8) initials jnrlst
%Control: editor formatted (1) identically to author
%Control: production of article title (-1) disabled
%Control: page (0) single
%Control: year (1) truncated
%Control: production of eprint (0) enabled
%merlin.mbs apsrev4-1.bst 2010-07-25 4.21a (PWD, AO, DPC) hacked
%Control: key (0)
%Control: author (0) dotless jnrlst
%Control: editor formatted (1) identically to author
%Control: production of article title (0) allowed
%Control: page (1) range
%Control: year (0) verbatim
%Control: production of eprint (0) enabled
%\begin{thebibliography}{61}%
%\end{thebibliography}%
\bibliography{main}




\section*{Supplementary}
Supplementary methods.

Supplementary movies:

Video 1: video of \textit{OT-II} hybridoma T-lymphocytes flowing in a microfluidic channel captured by TS-QPI; The cells are aligned at the center of the channel by hydrodynamic focusing. Optical path difference measured at four of the interrogation points on the rainbow flash is shown as a function of time in the right panels.

Video 2: video of \textit{SW-480} colon cancer epithelial cells flowing in a microfluidic channel captured by TS-QPI; The cells are aligned at the center of the channel by hydrodynamic focusing. Optical path difference measured at four of the interrogation points on the rainbow flash is shown as a function of time in the right panels.

% If you have acknowledgments, this puts in the proper section head.
\begin{acknowledgments}
This work was partially supported by NantWorks, LLC, Culver City, California. Ian K. Blaby was supported by the Office of Science (Biological and Environmental Research), U.S. Department of Energy (grant no. DE-FC02-02ER63421) to Sabeeha Merchant.
\end{acknowledgments}

\section*{Author contributions statement}

C.L.C., A.M., and B.J. conceived the classification method; A.M., C.L.C., and B.J. designed TS-QPI system and performed experiments; A.M. and C.L.C. collected the data and processed the signals; C.L.C., A.M., and B.J. designed, coded, and analyzed the image processing and the machine learning pipelines; A.M., L.C.T. and A.H. designed and fabricated the microfluidic channels; K.R.N. provided \textit{SW-480} and \textit{OT-II} cell samples; I.K.B. provided algal cell samples; C.L.C., A.M., B.J., I.K.B., and K.R.N. prepared the manuscript. 

\section*{Additional information}

The authors declare no competing financial interests.


\end{document}
%
% ****** End of file apstemplate.tex ******

